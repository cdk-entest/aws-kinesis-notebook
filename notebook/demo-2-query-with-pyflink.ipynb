{
  "metadata": {
    "name": "Demo 2 Query Data Stream KDA",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n## Generate Data Using Python SDK"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ipyflink\nimport datetime\nimport json\nimport random\nimport boto3\nimport time \n\n\nSTREAM_NAME \u003d \"stock-input-stream\"\nREGION \u003d \"ap-southeast-1\"\n\n\ndef get_data():\n    return {\n        \u0027event_time\u0027: datetime.datetime.now().isoformat(),\n        \u0027ticker\u0027: random.choice([\"BTC\",\"ETH\",\"BNB\", \"XRP\", \"DOGE\"]),\n        \u0027price\u0027: round(random.random() * 100, 2)}\n\n\ndef generate(stream_name, kinesis_client):\n    while True:\n        data \u003d get_data()\n        print(data)\n        kinesis_client.put_record(\n            StreamName\u003dstream_name,\n            Data\u003djson.dumps(data),\n            PartitionKey\u003d\"partitionkey\")\n        # time.sleep(1)\n\n\nif __name__ \u003d\u003d \u0027__main__\u0027:\n    generate(STREAM_NAME, boto3.client(\u0027kinesis\u0027, region_name\u003dREGION))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Create a In-Memory Table"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\nst_env.execute_sql(\"\"\"DROP TABLE IF EXISTS stock_table\"\"\")\nst_env.execute_sql(\"\"\"DROP TABLE IF EXISTS output_table\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\nst_env.execute_sql(\"\"\"\nCREATE TABLE stock_table (\n                ticker VARCHAR(6),\n                price DOUBLE,\n                event_time TIMESTAMP(3),\n                WATERMARK FOR event_time AS event_time - INTERVAL \u00275\u0027 SECOND\n\n              )\n              PARTITIONED BY (ticker)\n              WITH (\n                \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n                \u0027stream\u0027 \u003d \u0027stock-input-stream\u0027,\n                \u0027aws.region\u0027 \u003d \u0027ap-southeast-1\u0027,\n                \u0027scan.stream.initpos\u0027 \u003d \u0027LATEST\u0027,\n                \u0027format\u0027 \u003d \u0027json\u0027,\n                \u0027json.timestamp-format.standard\u0027 \u003d \u0027ISO-8601\u0027\n              ) \"\"\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n## Select and Filter \n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\nSELECT * FROM stock_table "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Tumbling Window\ncalculate average stock price group by ticker and window \n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\nSELECT \n        stock_table.ticker as ticker,\n        AVG(stock_table.price) AS avg_price,\n        TUMBLE_ROWTIME(stock_table.event_time, INTERVAL \u002710\u0027 second) as time_event\nFROM stock_table\nGROUP BY TUMBLE(stock_table.event_time, INTERVAL \u002710\u0027 second), stock_table.ticker;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n## Tumbling Window\ncount ticker by ticker and window \n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\nSELECT \n        stock_table.ticker as ticker, \n        COUNT(stock_table.ticker) AS ticker_count,\n        TUMBLE_ROWTIME(stock_table.event_time, INTERVAL \u00273\u0027 second) as time_event\nFROM stock_table\nGROUP BY TUMBLE(stock_table.event_time, INTERVAL \u00273\u0027 second), stock_table.ticker;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## Sliding Window \n\n- calculate average stock price group by ticker and window\n- window width 1 minute, update each 10 seconds\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\nSELECT \n        stock_table.ticker as ticker,\n        AVG(stock_table.price) AS avg_price,\n        HOP_ROWTIME(stock_table.event_time, INTERVAL \u002710\u0027 second, INTERVAL \u00271\u0027 minute) as hop_time\nFROM stock_table\nGROUP BY HOP(stock_table.event_time, INTERVAL \u002710\u0027 second, INTERVAL \u00271\u0027 minute), stock_table.ticker;\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n## Enable Checkpointing"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\nst_env.get_config().get_configuration().set_string(\n    \"execution.checkpointing.interval\", \"1min\"\n)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\nst_env.execute_sql(\"\"\"DROP TABLE IF EXISTS stock_output_table\"\"\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## Writing Results to Amazon S3"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\noutput_table_name \u003d \"stock_output_table\"\nbucket_name \u003d \"data-lake-demo-17072023\"\n\nst_env.execute_sql(\"\"\"CREATE TABLE {0} (\n                ticker VARCHAR(6),\n                price DOUBLE,\n                event_time TIMESTAMP(3)\n              )\n              PARTITIONED BY (ticker)\n              WITH (\n                  \u0027connector\u0027\u003d\u0027filesystem\u0027,\n                  \u0027path\u0027\u003d\u0027s3a://{1}/hehe/\u0027,\n                  \u0027format\u0027\u003d\u0027csv\u0027,\n                  \u0027sink.partition-commit.policy.kind\u0027\u003d\u0027success-file\u0027,\n                  \u0027sink.partition-commit.delay\u0027 \u003d \u00271 min\u0027\n              )\"\"\".format(table_name, bucket_name))"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\ntable_result \u003d st_env.execute_sql(\"INSERT INTO {0} SELECT * FROM {1}\".format(output_table_name, \"stock_table\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\nprint(table_result.get_job_client().cancel())"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n"
    }
  ]
}